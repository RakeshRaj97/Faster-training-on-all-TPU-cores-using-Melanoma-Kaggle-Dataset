{"cells":[{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","execution_count":1,"outputs":[{"output_type":"stream","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  5115  100  5115    0     0    626      0  0:00:08  0:00:08 --:--:--  1185\nUpdating... This may take around 2 minutes.\nUpdating TPU runtime to pytorch-nightly ...\nFound existing installation: torch 1.5.0\nUninstalling torch-1.5.0:\n  Successfully uninstalled torch-1.5.0\nFound existing installation: torchvision 0.6.0a0+35d732a\nUninstalling torchvision-0.6.0a0+35d732a:\nDone updating TPU runtime\n  Successfully uninstalled torchvision-0.6.0a0+35d732a\nCopying gs://tpu-pytorch/wheels/torch-nightly-cp37-cp37m-linux_x86_64.whl...\n\\ [1 files][109.0 MiB/109.0 MiB]  108.8 MiB/s                                   \nOperation completed over 1 objects/109.0 MiB.                                    \nCopying gs://tpu-pytorch/wheels/torch_xla-nightly-cp37-cp37m-linux_x86_64.whl...\n\\ [1 files][124.6 MiB/124.6 MiB]  112.2 MiB/s                                   \nOperation completed over 1 objects/124.6 MiB.                                    \nCopying gs://tpu-pytorch/wheels/torchvision-nightly-cp37-cp37m-linux_x86_64.whl...\n- [1 files][  2.4 MiB/  2.4 MiB]   89.6 MiB/s                                   \nOperation completed over 1 objects/2.4 MiB.                                      \nProcessing ./torch-nightly-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==nightly) (0.18.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==nightly) (1.18.5)\n\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n\u001b[31mERROR: kornia 0.3.1 has requirement torch==1.5.0, but you'll have torch 1.7.0a0+bfa9448 which is incompatible.\u001b[0m\n\u001b[31mERROR: allennlp 1.0.0 has requirement torch<1.6.0,>=1.5.0, but you'll have torch 1.7.0a0+bfa9448 which is incompatible.\u001b[0m\nInstalling collected packages: torch\nSuccessfully installed torch-1.7.0a0+bfa9448\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\nProcessing ./torch_xla-nightly-cp37-cp37m-linux_x86_64.whl\nInstalling collected packages: torch-xla\nSuccessfully installed torch-xla-1.6+354b242\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\nProcessing ./torchvision-nightly-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (1.7.0a0+bfa9448)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (7.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (1.18.5)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torchvision==nightly) (0.18.2)\nInstalling collected packages: torchvision\nSuccessfully installed torchvision-0.8.0a0+62e3fbd\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  libgfortran4 libopenblas-base\nThe following NEW packages will be installed:\n  libgfortran4 libomp5 libopenblas-base libopenblas-dev\n0 upgraded, 4 newly installed, 0 to remove and 59 not upgraded.\nNeed to get 8550 kB of archives.\nAfter this operation, 97.6 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgfortran4 amd64 7.5.0-3ubuntu1~18.04 [492 kB]\nGet:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenblas-base amd64 0.2.20+ds-4 [3964 kB]\nGet:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenblas-dev amd64 0.2.20+ds-4 [3860 kB]\nGet:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\nFetched 8550 kB in 17s (494 kB/s)\ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package libgfortran4:amd64.\n(Reading database ... 107745 files and directories currently installed.)\nPreparing to unpack .../libgfortran4_7.5.0-3ubuntu1~18.04_amd64.deb ...\nUnpacking libgfortran4:amd64 (7.5.0-3ubuntu1~18.04) ...\nSelecting previously unselected package libopenblas-base:amd64.\nPreparing to unpack .../libopenblas-base_0.2.20+ds-4_amd64.deb ...\nUnpacking libopenblas-base:amd64 (0.2.20+ds-4) ...\nSelecting previously unselected package libopenblas-dev:amd64.\nPreparing to unpack .../libopenblas-dev_0.2.20+ds-4_amd64.deb ...\nUnpacking libopenblas-dev:amd64 (0.2.20+ds-4) ...\nSelecting previously unselected package libomp5:amd64.\nPreparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\nUnpacking libomp5:amd64 (5.0.1-1) ...\nSetting up libomp5:amd64 (5.0.1-1) ...\nSetting up libgfortran4:amd64 (7.5.0-3ubuntu1~18.04) ...\nSetting up libopenblas-base:amd64 (0.2.20+ds-4) ...\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3 to provide /usr/lib/x86_64-linux-gnu/libblas.so.3 (libblas.so.3-x86_64-linux-gnu) in auto mode\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/liblapack.so.3 to provide /usr/lib/x86_64-linux-gnu/liblapack.so.3 (liblapack.so.3-x86_64-linux-gnu) in auto mode\nSetting up libopenblas-dev:amd64 (0.2.20+ds-4) ...\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/libblas.so to provide /usr/lib/x86_64-linux-gnu/libblas.so (libblas.so-x86_64-linux-gnu) in auto mode\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/liblapack.so to provide /usr/lib/x86_64-linux-gnu/liblapack.so (liblapack.so-x86_64-linux-gnu) in auto mode\nProcessing triggers for libc-bin (2.27-3ubuntu1) ...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!export XLA_USE_BF16=1","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install efficientnet_pytorch","execution_count":3,"outputs":[{"output_type":"stream","text":"Collecting efficientnet_pytorch\n  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet_pytorch) (1.7.0a0+bfa9448)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (1.18.5)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (0.18.2)\nBuilding wheels for collected packages: efficientnet-pytorch\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12419 sha256=8ed31bd74ee8ae260075b3353c38ca5a356f93d91fc51a730fbb31b518baa96b\n  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\nSuccessfully built efficientnet-pytorch\nInstalling collected packages: efficientnet-pytorch\nSuccessfully installed efficientnet-pytorch-0.6.3\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Import Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageFile\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nfrom sklearn import metrics\nfrom sklearn import model_selection\n\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nfrom joblib import Parallel, delayed\nimport efficientnet_pytorch\nimport albumentations\nfrom tqdm import tqdm\n\nImageFile.LOAD_TRUNCATED_IMAGES = True","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Kfolds"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/jpeg-melanoma-256x256/train.csv\")\ndf[\"kfold\"] = -1\ndf = df.sample(frac=1).reset_index(drop=True)\ny = df.target.values\nkf = model_selection.StratifiedKFold(n_splits=8)\nfor fold_, (train_idx, test_idx) in enumerate(kf.split(X=df, y=y)):\n    df.loc[test_idx, \"kfold\"] = fold_\ndf.to_csv(\"train_folds.csv\", index=False)","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataloader"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ClassificationDataset:\n    def __init__(self, image_paths, targets, resize, augmentations=None):\n        self.image_paths = image_paths\n        self.targets = targets\n        self.resize = resize\n        self.augmentations = augmentations\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, item):\n        image = Image.open(self.image_paths[item])\n        targets = self.targets[item]\n        if self.resize is not None:\n            image = image.resize(\n                (self.resize[1], self.resize[0]), resample=Image.BILINEAR\n            )\n        image = np.array(image)\n        targets = np.array(targets)\n        if self.augmentations is not None:\n            augmented = self.augmentations(image=image)\n            image = augmented[\"image\"]\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        return {\n            \"image\": torch.tensor(image, dtype=torch.float),\n            \"targets\": torch.tensor(targets, dtype=torch.long),\n        }","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class EfficientNet(nn.Module):\n    def __init__(self):\n        super(EfficientNet, self).__init__()\n        self.base_model = efficientnet_pytorch.EfficientNet.from_pretrained(\n            'efficientnet-b7'\n        )\n        self.base_model._fc = nn.Linear(\n            in_features=2560,\n            out_features=1,\n            bias=True\n        )\n\n    def forward(self, image, targets):\n        out = self.base_model(image)\n        loss = nn.BCEWithLogitsLoss()(out, targets.view(-1, 1).type_as(out))\n        return out, loss","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(data_loader=None, model=None, optimizer=None, scheduler=None, device=None):\n    model.train()\n    para_loader = pl.ParallelLoader(data_loader, [device])\n    tk0 = para_loader.per_device_loader(device)\n    for b_idx, data in enumerate(tk0):\n        for key, value in data.items():\n            data[key] = value.to(device)\n        optimizer.zero_grad()\n        _, loss = model(**data)\n        loss.backward()\n        xm.optimizer_step(optimizer, barrier=True)\n        scheduler.step(loss)\n        return loss.item()","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Eval function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(\n        data_loader,\n        model,\n        device,\n):\n    model.eval()\n    with torch.no_grad():\n        para_loader = pl.ParallelLoader(data_loader, [device])\n        tk0 = para_loader.per_device_loader(device)\n        for b_idx, data in enumerate(tk0):\n            for key, value in data.items():\n                data[key] = value.to(device)\n            predictions, loss = model(**data)\n            predictions = torch.sigmoid(predictions)\n    return loss.item(), predictions\n","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"def run(fold):\n    training_data_path = \"../input/siic-isic-224x224-images/train/\"\n    df = pd.read_csv(\"./train_folds.csv\")\n    device = xm.xla_device()\n    epochs = 5\n    train_bs = 32\n    valid_bs = 16\n    mean = (0.485, 0.456, 0.406)\n    std = (0.229, 0.224, 0.225)\n    \n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    model = EfficientNet()\n    model.to(device)\n    \n    train_aug = albumentations.Compose(\n        [\n            albumentations.Normalize(\n                mean, \n                std, \n                max_pixel_value=255.0, \n                always_apply=True\n            ),\n            albumentations.ShiftScaleRotate(\n                shift_limit=0.0625, \n                scale_limit=0.1, \n                rotate_limit=15\n            ),\n            albumentations.Flip(p=0.5)\n        ]\n    )\n\n    valid_aug = albumentations.Compose(\n        [\n            albumentations.Normalize(\n                mean, \n                std, \n                max_pixel_value=255.0,\n                always_apply=True\n            )\n        ]\n    )\n    \n    train_images = df_train.image_name.values.tolist()\n    train_images = [\n        os.path.join(training_data_path, i + \".png\") for i in train_images\n    ]\n    train_targets = df_train.target.values\n\n    valid_images = df_valid.image_name.values.tolist()\n    valid_images = [\n        os.path.join(training_data_path, i + \".png\") for i in valid_images\n    ]\n    valid_targets = df_valid.target.values\n\n    train_dataset = ClassificationDataset(\n        image_paths=train_images,\n        targets=train_targets,\n        resize=None,\n        augmentations=train_aug\n    )\n    \n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n      train_dataset,\n      num_replicas=xm.xrt_world_size(),\n      rank=xm.get_ordinal(),\n      shuffle=True\n    )\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=train_bs,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=0\n    )\n    \n    valid_dataset = ClassificationDataset(\n        image_paths=valid_images,\n        targets=valid_targets,\n        resize=None,\n        augmentations=valid_aug\n    )\n    \n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n      valid_dataset,\n      num_replicas=xm.xrt_world_size(),\n      rank=xm.get_ordinal(),\n      shuffle=True\n    )\n    \n    \n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=valid_bs,\n        sampler=valid_sampler,\n        drop_last=False,\n        num_workers=0\n    )\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer,\n        patience=3,\n        threshold=0.001,\n        mode=\"min\"\n    )\n    \n    for epoch in range(epochs):\n        training_loss = train(\n            data_loader=train_loader,\n            model=model,\n            optimizer=optimizer,\n            device=device,\n            scheduler=scheduler,\n        )\n        \n        valid_loss, predictions = evaluate(\n            valid_loader,\n            model,\n            device,\n        \n        )\n        \n        xm.master_print(f\"Epoch = {epoch}, LOSS = {valid_loss}\")\n        gc.collect()\n    ","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = run(0)\n    \n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')\n","execution_count":null,"outputs":[{"output_type":"stream","text":"Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b7-dcc49843.pth\nDownloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b7-dcc49843.pth\nDownloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b7-dcc49843.pth\nDownloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b7-dcc49843.pth\nDownloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b7-dcc49843.pth\nDownloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b7-dcc49843.pth\nDownloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b7-dcc49843.pth\nDownloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b7-dcc49843.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=266860719.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"980fa0da9bbd4a799e11af44351ebfc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=266860719.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2038e765d51f4c9fa51877c834d28f40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=266860719.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fbbe55d56aa4ec4a51a18ae2b9eac3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=266860719.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b58d83b0372e49aa808f5cef4b7c2236"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=266860719.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a3a7cb7b28a40d8aadaad9bd89420aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=266860719.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a2ea53c8c494e40a09337b61915b4f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=266860719.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d23a10c0743b45d49f33b0f9995145e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=266860719.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abc80af30bc14d0ea7b45e296f87a40a"}},"metadata":{}},{"output_type":"stream","text":"\n\nLoaded pretrained weights for efficientnet-b7\nLoaded pretrained weights for efficientnet-b7\n\n\n\nLoaded pretrained weights for efficientnet-b7\nLoaded pretrained weights for efficientnet-b7\nLoaded pretrained weights for efficientnet-b7\n\nLoaded pretrained weights for efficientnet-b7\n\n\nLoaded pretrained weights for efficientnet-b7\nLoaded pretrained weights for efficientnet-b7\nEpoch = 0, LOSS = 0.7496073246002197\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}